# Отчет по дз1

## Немного комментариев к дз
### datasets
Заменил `eriktks/conll2003` на `lhoestq/conll2003`, т.к. с `datasets>4.0.0` нельзя использовать скрипты 

[link](https://discuss.huggingface.co/t/cannot-load-conll2003/169142)

### wandb
wandb не использовал т.к. он недоступен в России

### Модули
Все дополнительные модули лежат в папке с дз
```
hw1/compressed_embed.py
hw1/distillation_trainer.py
hw1/utils.py
```

## Задание 1

**Задача:** Токенизируйте весь датасет и для каждого текста выравните токены с метками так, чтобы каждому токену соответствовала одна метка. При этом важно сохранить нотацию BIO. И не забудьте про специальные токены!

Функция `align_labels_with_tokens` реализована в файле `utils.py`. 

В нее передаются метки для слов и список `word_ids` от токенизатора
про `word_ids` [здесь](https://huggingface.co/docs/transformers/v5.2.0/en/main_classes/tokenizer#transformers.BatchEncoding.word_ids)

Она сопоставляет каждому токену соответствующую метку, используя следующие правила:
1.  Для специальных токенов (`word_id is None`) устанавливается метка `-100`, чтобы функция потерь игнорировала их
2.  Для первого токена слова (`word_id != prev_word_id`) используется метка
3.  Для последующих токенов того же слова (`word_id == prev_word_id`) метка выставляется по следующим правилам:
    *   Если метка была началом сущности, то продолжению присваивается соответствующая метка
    *   В противном случае продолжению также присваивается `O`


## Задание 2

**Задача:** Дообучите `bert-base-cased` на нашем датасете с помощью обычного fine-tuning. У вас должно получиться хотя бы 0.9 F1 на тестовой выборке. Заметьте, что чем выше качество большой модели, тем лучше будет работать дистиллированный ученик. Для обучения можно использовать `Trainer` из Hugging Face.

Обучение проводилось с помощью `Trainer` из библиотеки Transformers. Гиперпараметры:
```python
TrainingArguments(
    ...

    learning_rate=5e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=5,
    max_grad_norm=3,
    weight_decay=1e-6,

    warmup_steps=1000,
    lr_scheduler_type="linear",
    optim="adamw_torch",

    ...
)
```


**Результат:** 
```
Epoch	Training Loss	Validation Loss	F1
1	No log	0.081887	0.862748
2	0.527516	0.063979	0.924386
3	0.061097	0.056666	0.922799
4	0.030874	0.054040	0.936367
5	0.012369	0.053443	0.937845
```

на тесте
```
{'eval_loss': 0.14250044524669647,
 'eval_f1': 0.9020583190394512,
 'eval_runtime': 4.8002,
 'eval_samples_per_second': 719.34,
 'eval_steps_per_second': 22.499,
 'epoch': 5.0}
```

## Задание 3

**Задача:** Напишите класс-обертку над слоем эмбеддингов, который реализует факторизацию на две матрицы, и дообучите факторизованную модель. Заметьте, обе матрицы можно инициализировать с помощью SVD разложения, чтобы начальное приближение было хорошим. Это сэкономит очень много времени на дообучении. С рангом разложения $H = 64$ у вас должно получиться F1 больше 0.87.


Был реализован класс `CompressedWordEmbedding` в файле `compressed_embed.py`

Кол-во параметров модели сократилось с **107.7M** до **87.4M**

**Результат:**
```
Epoch	Training Loss	Validation Loss	F1
1	No log	0.102922	0.872891
2	0.179485	0.098536	0.899959
3	0.049354	0.092820	0.900518
4	0.027929	0.085501	0.912240
5	0.011507	0.082511	0.927876
```

на тесте
```
{'eval_loss': 0.18780745565891266,
 'eval_f1': 0.8834080717488789,
 'eval_runtime': 4.7949,
 'eval_samples_per_second': 720.14,
 'eval_steps_per_second': 22.524,
 'epoch': 5.0}
```

## Задание 4

**Задача:** Реализуйте метод дистилляции знаний, изображенный на картинке. Для подсчета ошибки между предсказаниями ученика и учителя используйте KL-дивергенцию [`nn.KLDivLoss(reduction="batchmean")`](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html) (обратите внимание на вормат ее входов). Для получения итоговой ошибки суммируйте мягкую ошибку с жесткой.   
В качестве учителя используйте дообученный BERT из задания 2. В качестве ученика возьмите необученную модель с размером __не больше 20M__ параметров. Вы можете использовать факторизацию матрицы эмбеддингов для уменьшения числа параметров. Если вы все сделали правильно, то на тестовой выборке вы должны получить значение F1 не меньше 0.7. Вам должно хватить примерно 20к итераций обучения для этого. Если у вас что-то не получается, то можно ориентироваться на статью про [DistilBERT](https://arxiv.org/abs/1910.01108) и на [эту статью](https://www.researchgate.net/publication/375758425_Knowledge_Distillation_Scheme_for_Named_Entity_Recognition_Model_Based_on_BERT).


Была создана модель на основе `bert-base-cased`, но с уменьшенными размерами для достижения лимита в 20M параметров, практически все параметры уменьшались в два раза кроме intermediate_size (уменьшил в 3 раза)
    *   `hidden_size`: 384 (вместо 768)
    *   `num_hidden_layers`: 6 (вместо 12)
    *   `num_attention_heads`: 6 (вместо 12)
    *   `intermediate_size`: 1024 (вместо 3072)

Результирующее число параметров: **~19.62M**

Был написан класс `DistillationTrainer` в файле `distillation_trainer.py`

*   Логиты учителя "смягчаются" с помощью температуры (`temperature=2.0`). Логиты ученика также масштабируются на температуру. Потери считаются как `KLDivLoss` между двумя распределениями. Результат умножается на `temperature**2`
*  `total_loss = soft_loss(kl_div) + hard_loss(cross_entropy)`

Гиперпараметры
```python
TrainingArguments(
    ...

    learning_rate=3e-4,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=10,
    max_grad_norm=3,
    weight_decay=1e-6,

    warmup_steps=1000,
    lr_scheduler_type="linear",
    optim="adamw_torch",

    ...
)

trainer = DistillationTrainer(
    ...
    temperature=2
)
```

**Результаты:**
```
Epoch	Training Loss	Validation Loss	F1
1	No log	27.117105	0.747345
2	8.168546	27.904615	0.717997
3	10.081097	29.386084	0.714377
4	11.079782	28.393822	0.740652
5	9.572203	28.232073	0.739407
6	7.780821	27.703833	0.752839
7	6.330758	27.075539	0.760812
8	5.644159	25.472563	0.766841
9	5.644159	25.510351	0.767374
10	4.569647	25.061512	0.772351
```

на тесте
```
{'eval_loss': 30.057355880737305,
 'eval_f1': 0.7005015727280455,
 'eval_runtime': 5.4912,
 'eval_samples_per_second': 628.83,
 'eval_steps_per_second': 19.668,
 'epoch': 10.0}
```
